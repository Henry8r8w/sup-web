[{"id":0,"href":"/docs/table-of-content/","title":"Table of Content","section":"Docs","content":" üìö Topics # Network Economics Game Theory Modeling Pricing Optimization Market Design Code Simulations # üìù Blog # Visit Blog Page ML Engineering\nüìö Topics # Probabilistic Machine Learning Computer Vision Visual Search Information Theory Signals, System and Inference Causal ML Recommender System Code Simulations # üìù Blog # Visit Blog Page "},{"id":1,"href":"/docs/blog/year-2025/","title":"Year 2025","section":"Blog Page","content":" Spring 2025 # I aint got no job, so I gotta learn some stuff to get my stuff together. I want MLE job and potentially finacne. SO much crazy amout of topics to study. Creating projects seemed hard, but we\u0026rsquo;ll go from there\n"},{"id":2,"href":"/docs/topics/network-economics/matching-in-network/","title":"Matching in Network","section":"Networks Economics","content":" Intro: Traditional Decentralized Trade # Market Clearing Mechanism: Walrasian Auctioneer # Prices are set by a central auctioneer to clear the market.\nAgents act as price takers; no strategic pricing or networked trade.\nAssumes:\nAnonymous transactions\nGoods and money flow through markets, not individual-to-individual (non-market mechanisms).\nHowever, in reality:\nMany trades happen through non-market mechanisms (e.g., labor, dating, organ exchange). Decentralized Matching # Modern matching theory models decentralized trade using bipartite graphs.\nExample:\nMedical residency matching (n-to-1) they rank yours , you rank yours (linnk) Bipartite Graphs and Matching Theory # A bipartite graph is a graph where:\nNodes are divided into two disjoint sets: A and B.\nEdges connect nodes only across sets, never within.\nKey Concepts # Perfect Matching # A perfect matching is when every node in set A is matched to a unique node in set B and vice versa.\nMarriage Theorem (Hall\u0026rsquo;s Theorem) # A bipartite graph has a perfect matching iff for every subset $S‚äÜAS \\subseteq A$, the set of neighbors $N(S)‚äÜBN(S) \\subseteq B$ satisfies:\n$$|N(S)| \\geq |S| ]$$\nIf this condition is violated, the subset S is called a constricted set.\nConstricted Set # Definition:\nA subset S‚äÜAS \\subseteq A is constricted if the number of neighbors it connects to in BB is strictly less than ‚à£S‚à£|S|. That is, ‚à£N(S)‚à£\u0026lt;‚à£S‚à£|N(S)| \u0026lt; |S|.\nImplication: No matter how you match, at least one agent in SS will remain unmatched. This is a structural constraint‚Äînot a flaw of the algorithm.\nExample Graph (Non-Perfect Matching) # graph LR\rsubgraph A [ ]\ra1((a‚ÇÅ))\ra2((a‚ÇÇ))\ra3((a‚ÇÉ))\rend\rsubgraph B [ ]\rb1((b‚ÇÅ))\rb2((b‚ÇÇ))\rb3((b‚ÇÉ))\rend\ra1 --\u0026gt; b1\ra2 --\u0026gt; b1\ra2 --\u0026gt; b2\ra2 --\u0026gt; b3\ra3 --\u0026gt; b1 Now, consider the subset $S={a1,a3}S = {a_1, a_3}$\nNeighbors $N(S)={b1}N(S) = { b_1 }$\n$‚à£N(S)‚à£=1\u0026lt;2=‚à£S‚à£|N(S)| = 1 \u0026lt; 2 = |S|$\nExample Graph (Perfect Matching) # graph LR\rsubgraph A [ ]\ra1((a‚ÇÅ))\ra2((a‚ÇÇ))\ra3((a‚ÇÉ))\rend\rsubgraph B [ ]\rb1((b‚ÇÅ))\rb2((b‚ÇÇ))\rb3((b‚ÇÉ))\rend\ra1 --\u0026gt; b1\ra2 --\u0026gt; b1\ra2 --\u0026gt; b2\ra2 --\u0026gt; b3\ra3 --\u0026gt; b3 Centralized vs Decentralized # Centralized Decentralized Central auctioneer (Walrasian) Peer-to-peer interactions Strategy-proof mechanisms Strategic misreporting possible One-to-one or n-to-1 matching Risk of unmatched agents Strategy-Proof Matching # Goal of market design: prevent strategic manipulation (e.g., lying to get better outcome).\nAlgorithms like Deferred Acceptance (DA) are strategy-proof on one side.\nMatch Making Algorithm # The basic model # Setup # Agents\nsome number of seller: B_s where i in s some number of Buyers: N_b, where j in b Action space\nSeller puts house up for sale for $p_i ‚â• 0$ Buyer can buy at most one house Preferences\nSellers value house at 0 (this is a simplification w.l.o.g.) Buyers have some private valuation vij for each house Payoffs\nSellers want to maximize selling price p_i Buyers want to maximize vij ‚àí pi (their value - sale price for i hosue) Equilibrium Market-clearing prices set such that no player desires to deviate Sounds like Nash eqm, but not exactly the same concept To solve for equilibrium\nGiven a buyer j, define a set of preferred sellers as those that maximize payoff Preferred-Seller Graph # Given price vector s, construct preferred seller graph as follows There exists a link between buyer and seller if buying from that seller maximizes utility With discrete prices, each is not nessesary unique, so this will not (in geenral) lead to a one-to-one matching Notice that thre resulting graph is condition on p In general, we are intersted in making statements about mapping from p to graph Example # Agents\nthree sellers / houses: a, b, c Buyers\rSellers\ra\rb\rc\rx\r12\r4\r2\ry\r8\r7\r6\rz\r7\r5\r2\r- Three buyers with valuation as follows\r- value vecotr: - x: (a:12, b: 4, c : 2)\r- y: (a:8, b: 7, : c 6)\r- z: (a:7, b:5, c: 2)\r- Three sellers: abc with price vectors as follows (which we will be testing)\r- price vector: (5, 2, 0);(2,1,0); (3,1, 0)\rSellers action: max{p_i} Buyers action: { vij ‚àí pi} note:\nthis is a naive process, because equilibirum could exit with more than one kind of price vector Market CLearing Prices # If the resulting preferred-seller graph has perfect matching Theroem: For any set of buyer valuations, there exists a set of market-clearing prices But for only some set of selll prices, there exist a set of market-clearing valuation Efficiency # effiicney can be biased towards seller or buyer, whihc is something that we can control (simuilate)\ntrasnfer utility matching model $$ \\text{Welfare} = \\underbrace{\\sum_i p_i}{\\text{Seller utility}} + \\underbrace{\\sum_j (v{ij} - p_j)}_{\\text{Buyer utility}} $$ Efficent is max{Welfare}\nTheorem\nFor any set of market-clearing prices, a perfect matching in the resulting preferred-seller graph has the maximum total valuation of any assignment of buyers to sellers Finding Market-Clearnign Prices # Without a price vecotr given, how would we have market clearing rpices?\nTO make compairons\ntotal welfare consumer and seller surplus in the total welfare Algorithm At iteration 1, set all prices to 0 At each iteration, - 1 Re-scale all prices so that smallest one is 0 - 2 Construct preferred-seller network - 3 If there is a perfect matching (no constricted set), then we‚Äôre done - 4 If there is a constricted set of buyers, raise price of neighbors of buyers in that set each by 1 unit Pareto optimailty # This is a type of ascending price auction Start at lowest price that will get all sellers to participate In general, it will find a low price\nIn most cases, we can uniformly raise the price by some amount and still get the same transactions Decreasing price auction will in general find a higher price\n"},{"id":3,"href":"/docs/topics/probabilistic-ml/monte-carlo/","title":"Monte Carlo","section":"Probabilistic ML","content":" üèÉ‚Äç‚ôÇÔ∏èThe Steps to Obtain Our Quantity of Interest # Step 1 : Generate $S_n$ from the distribution $X_1, \u0026hellip;X_n$ # Various Methods:\nMarkov Chain Monto Carlo Binomial distribution Step 2: Given the Random Samples on $f(x_{s})$ we approximate the empirical distribution of ${f(x_s)}^n_1$ # Monto Carlo approximation\nDeveloped in statistical physics during atomic bomb period Named after the plush gambling casino The expected values are what typically asked\n$$ \\mathbb{E}[f(X)] = \\int f(s) p(x)dx \\approx \\frac{1}{N}\\sum^{N}{s = 1}f(x{s}), \\text{ where } s \\in N, x_{s} \\sim p(x) $$ aka. Monte Carlo Integration (canonical form)\nüòú The Advantage of Monto Carlo Over Numerical Integration # Numerical integration (like Simpson\u0026rsquo;s Rule, Trapezoidal Rule, or grid-based methods) evaluates the function f(x) at uniform intervals across the entire domain‚Äîeven in regions where the function contributes very little or nothing to the integral (e.g., $p(x) \\approx 0$)\nIn contrast, Monte Carlo integration samples values of x according to the probability distribution p(x). This ensures that: Most samples fall in high-probability regions Note: In short, Monte Carlo is simply just an expected value calculation on top of random function $f\\left( \\cdot \\right)$ sample [! hint] It was never the formula that made Monto Carlo unique, but from the analytical standpoint; it is a useful perspective to estimate $f(x)$ based on important weights; and it fits well on high dimension space\nüìòExample # $\\Delta$ Example 1. Change of Variables, the MC way # $$y = f(x)$$ $$x \\sim \\text{Uni(-1,1)}, y = x^2$$ Approximate p(y) by drawing many samples from $p(x)$ and create distribution\ninsert mcapproximated code simulation\nFigure 2.19 Estimating œÄ by Monte Carlo integration. Blue points are inside the circle, red crosses are outside.\nüé§ Example 2. Estimating $\\pi$ by Monte Carlo Integration # Function definition: $$f(x,y) = \\begin{cases} 1 \u0026amp; (x^2 + y^2 \\leq r^2) \\ 0 \u0026amp; \\text{otherwise} \\end{cases} $$ We know that $\\pi = A / r^2$. Here we define the raw Area Integral of Circle as $$ A = \\int^{r}{-r} \\int^{r}{-r} \\mathbb{I}(x^2 +y^2 \\leq r^2) ,dx,dy $$\n$\\mathbb{I}()$ is an indicator function that is going to evaluate 1 or 0 based on the condition Now, let $p(x)$ and $p(y)$ be defined as part of the uniform distribution on $[-r, r]$ range. Therefore probabilities are as $p(x) = p(y) =\\frac{1}{2r}$ The Monte Carlo Integral $$ A = (2r)(2r) \\iint f(x, y) \\cdot p(x) \\cdot p(y), dx, dy \\ \\approx 4r^2 \\cdot \\frac{1}{S} \\sum_{s=1}^S f(x_s, y_s) $$\nNote:\nThe $4r^2$ factor scales back up after uniform sampling over $[-r, r]^2$, whose joint density is $p(x)p(y) = \\frac{1}{4r^2}$. Since uniform sampling gives us $\\mathbb{E}[f(x, y)] = \\frac{1}{4r^2} \\iint f(x, y), dx, dy$, we multiply by $4r^2$ to recover the original integral. üéØExample 3. Accuracy of Monte Carlo # Relatinship of Normail Distrbution to Monte Carlo\n$$(\\hat{\\mu} - \\mu) \\rightarrow \\mathcal{N}(0, \\frac{\\sigma^2}{S})$$ where\nVariance is defined as $$\\sigma^2 = \\mathbb{E}[f(X)^2] -\\mathbb{E}[f(X)]^2 \\Leftrightarrow \\frac{1}{S}\\sum^S_{1}(f(x_{s} - \\hat{\\mu}))^2$$\nConfidence Interval is defined as\n$$ P\\left{ \\mu - 1.96 \\cdot \\frac{\\hat{\\sigma}}{\\sqrt{S}} \\leq \\hat{\\mu} \\leq \\mu + 1.96 \\cdot \\frac{\\hat{\\sigma}}{\\sqrt{S}} \\right} \\approx 0.95 $$\nReference # [1] K. P. Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2023. - ch 2.7\n"},{"id":4,"href":"/docs/codes/","title":"Code Simulations","section":"Docs","content":" Code Simulations # This page collects simulations I\u0026rsquo;ve built to explore and explain technical concepts in machine learning, embedded systems, and algorithms. Each simulation links to its GitHub repository, where full code and documentation are maintained.\nFeatured Simulations # Pi Estimation (Monte Carlo) # Utilize $f_x(.)$ and calculated expectation during iterative randomized sampling\nTopics: Probabilistic ML üìÇ GitHub Repo Tag: Educational Signal Reconstruction (FFT vs DTFT) # Compare frequency-domain techniques using real audio snippets or test signals.\nTopics: Signal Processing üìÇ GitHub Repo Tag: Utility-Oriented Navigation # Each project is maintained in its own GitHub repo. More context and theory behind these simulations can be found in the Topics section. tags: Educational, Utility-Oriented, Exploratory\n"},{"id":5,"href":"/docs/mylife/","title":"My Life","section":"Docs","content":" Workout # Movies I watched: Review # Books I read # Talks I enjoyed # "},{"id":6,"href":"/docs/topics/network-economics/","title":"Networks Economics","section":"Topics","content":" List of Topics Covered # matching in network\n(\u0026hellip;) "},{"id":7,"href":"/docs/topics/probabilistic-ml/","title":"Probabilistic ML","section":"Topics","content":" List of Topics Covered # Monte Carlo Method\n(\u0026hellip;) Reference # K. P. Murphy, Probabilistic Machine Learning: An Introduction, MIT Press, 2023.\n"}]