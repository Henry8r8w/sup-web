[{"id":0,"href":"/docs/table-of-content/","title":"Table of Content","section":"Docs","content":" # [ML Engineering](Introduction/ML Engineering/)\nAcerbo datus maxime # Astris ipse furtiva # Est in vagis et Pittheus tu arge accipiter regia iram vocatur nurus. Omnes ut olivae sensit arma sorori deducit, inesset crudus, ego vetuere aliis, modo arsit? Utinam rapta fiducia valuere litora adicit cursu, ad facies\nSuis quot vota # Ea furtique risere fratres edidit terrae magis. Colla tam mihi tenebat: miseram excita suadent es pecudes iam. Concilio quam velatus posset ait quod nunc! Fragosis suae dextra geruntur functus vulgata.\nTempora nisi nunc # Lorem markdownum emicat gestu. Cannis sol pressit ducta. Est Idaei, tremens ausim se tutaeque, illi ulnis hausit, sed, lumina cutem. Quae avis sequens!\nvar panel = ram_design;\rif (backup + system) {\rfile.readPoint = network_native;\rsidebar_engine_device(cell_tftp_raster,\rdual_login_paper.adf_vci.application_reader_design(\rgraphicsNvramCdma, lpi_footer_snmp, integer_model));\r}\rLocis suis novi cum suoque decidit eadem # Idmoniae ripis, at aves, ali missa adest, ut et autem, et ab?\n"},{"id":1,"href":"/docs/blog/2025---2nd-pt/","title":"2025 2nd Pt","section":"Blog","content":" Sup # "},{"id":2,"href":"/docs/codes/","title":"Codes","section":"Docs","content":"This folder includes code experiments for Monte Carlo methods, Bayesian inference, and neural network calibration in embedded contexts.\n"},{"id":3,"href":"/docs/projects/","title":"Projects","section":"Docs","content":"This section organizes projects that i have previously done\n"},{"id":4,"href":"/docs/papers/","title":"Papers","section":"Docs","content":" üìö 2000s # üìö 2010s # shazam üìö 2020s # "},{"id":5,"href":"/docs/codes/monte-carlo/","title":"Monte Carlo","section":"Codes","content":" Monte Carlo Approximation # üèÉ‚Äç‚ôÇÔ∏èThe Steps to Obtain Our Quantity of Interest # Step 1 : Generate $S_n$ from the distribution $X_1, \u0026hellip;X_n$ # Various Methods:\nMarkov Chain Monto Carlo Binomial distribution Step 2: Given the Random Samples on $f(x_{s})$ we approximate the empirical distribution of ${f(x_s)}^n_1$ # Monto Carlo approximation\nDeveloped in statistical physics during atomic bomb period Named after the plush gambling casino The expected values are what typically asked\n$$ \\mathbb{E}[f(X)] = \\int f(s) p(x)dx \\approx \\frac{1}{N}\\sum^{N}{s = 1}f(x{s}), \\text{ where } s \\in N, x_{s} \\sim p(x) $$ aka. Monte Carlo Integration (canonical form)\nüòú The Advantage of Monto Carlo Over Numerical Integration # Numerical integration (like Simpson\u0026rsquo;s Rule, Trapezoidal Rule, or grid-based methods) evaluates the function f(x) at uniform intervals across the entire domain‚Äîeven in regions where the function contributes very little or nothing to the integral (e.g., $p(x) \\approx 0$)\nIn contrast, Monte Carlo integration samples values of x according to the probability distribution p(x). This ensures that: Most samples fall in high-probability regions Note: In short, Monte Carlo is simply just an expected value calculation on top of random function $f\\left( \\cdot \\right)$ sample [! hint] It was never the formula that made Monto Carlo unique, but from the analytical standpoint; it is a useful perspective to estimate $f(x)$ based on important weights; and it fits well on high dimension space\nüìòExample # $\\Delta$ Example 1. Change of Variables, the MC way # $$y = f(x)$$ $$x \\sim \\text{Uni(-1,1)}, y = x^2$$ Approximate p(y) by drawing many samples from $p(x)$ and create distribution\ninsert mcapproximated code simulation\nFigure 2.19 Estimating œÄ by Monte Carlo integration. Blue points are inside the circle, red crosses are outside.\nüé§ Example 2. Estimating $\\pi$ by Monte Carlo Integration # Function definition: $$f(x,y) = \\begin{cases} 1 \u0026amp; (x^2 + y^2 \\leq r^2) \\ 0 \u0026amp; \\text{otherwise} \\end{cases} $$ We know that $\\pi = A / r^2$. Here we define the raw Area Integral of Circle as $$ A = \\int^{r}{-r} \\int^{r}{-r} \\mathbb{I}(x^2 +y^2 \\leq r^2) ,dx,dy $$\n$\\mathbb{I}()$ is an indicator function that is going to evaluate 1 or 0 based on the condition Now, let $p(x)$ and $p(y)$ be defined as part of the uniform distribution on $[-r, r]$ range. Therefore probabilities are as $p(x) = p(y) =\\frac{1}{2r}$ The Monte Carlo Integral $$ A = (2r)(2r) \\iint f(x, y) \\cdot p(x) \\cdot p(y), dx, dy \\ \\approx 4r^2 \\cdot \\frac{1}{S} \\sum_{s=1}^S f(x_s, y_s) $$\nNote:\nThe $4r^2$ factor scales back up after uniform sampling over $[-r, r]^2$, whose joint density is $p(x)p(y) = \\frac{1}{4r^2}$. Since uniform sampling gives us $\\mathbb{E}[f(x, y)] = \\frac{1}{4r^2} \\iint f(x, y), dx, dy$, we multiply by $4r^2$ to recover the original integral. üéØExample 3. Accuracy of Monte Carlo # Relatinship of Normail Distrbution to Monte Carlo\n$$(\\hat{\\mu} - \\mu) \\rightarrow \\mathcal{N}(0, \\frac{\\sigma^2}{S})$$ where\nVariance is defined as $$\\sigma^2 = \\mathbb{E}[f(X)^2] -\\mathbb{E}[f(X)]^2 \\Leftrightarrow \\frac{1}{S}\\sum^S_{1}(f(x_{s} - \\hat{\\mu}))^2$$\nConfidence Interval is defined as\n$$ P\\left{ \\mu - 1.96 \\cdot \\frac{\\hat{\\sigma}}{\\sqrt{S}} \\leq \\hat{\\mu} \\leq \\mu + 1.96 \\cdot \\frac{\\hat{\\sigma}}{\\sqrt{S}} \\right} \\approx 0.95 $$\n"},{"id":6,"href":"/docs/table-of-content/economics/","title":"Economics","section":"Table of Content","content":" üìö Topics # Network Economics Game Theory Modeling Pricing Optimization Market Design Code Simulations # üìù Blog # Visit Blog Page "},{"id":7,"href":"/docs/table-of-content/ml-engineering/","title":"ML Engineering","section":"Table of Content","content":" üìö Topics # Probabilistic Machine Learning Computer Vision Visual Search Information Theory Signals, System and Inference Causal ML Recommender System Code Simulations # üìù Blog # Visit Blog Page "}]